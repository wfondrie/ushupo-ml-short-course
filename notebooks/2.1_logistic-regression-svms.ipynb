{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9751d648",
   "metadata": {},
   "source": [
    "# Lab 2.1: Using logistic regression to predict the quality of mass spectrometry experiments.\n",
    "\n",
    "As we learned in the previous lecture, logistic regression is a supervised learning method for classification when you have two classes. In this lab, we're going to explore one use case of logistic regression applied to mass spectrometry data: predicting the quality of an LC-MS experiment. In fact, we'll actually be recreating some of the work presented by [Amidan et al](https://pubs.acs.org/doi/10.1021/pr401143e).\n",
    "\n",
    "Before we begin our analysis though, we need to first import a few Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9304ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/justinsanders/Documents/noble lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m                \u001b[39m# For working with tabular data.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m    \u001b[39m# For plotting data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m              \u001b[39m# For theming our plots.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     LogisticRegression,            \u001b[39m# To use logistic regression.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     LogisticRegressionCV,          \u001b[39m# Automatically select hyperparameters using cross-validation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler \u001b[39m# Used to normalize the features\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd                # For working with tabular data.\n",
    "import matplotlib.pyplot as plt    # For plotting data\n",
    "import seaborn as sns              # For theming our plots.\n",
    "from sklearn.svm import SVC        # to use an SVM\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,            # To use logistic regression.\n",
    "    LogisticRegressionCV,          # Automatically select hyperparameters using cross-validation\n",
    ") \n",
    "from sklearn.preprocessing import StandardScaler # Used to normalize the features\n",
    "\n",
    "# These will calculate our ROC and precision-recall curve metrics.\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    RocCurveDisplay,\n",
    "    average_precision_score,\n",
    "    PrecisionRecallDisplay,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "\n",
    "# Make our plots look nice:\n",
    "sns.set(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# These variables define the paths to our data:\n",
    "metadata_csv = \"../data/quality/metadata.csv\"  # A summary of the data\n",
    "train_csv = \"../data/quality/training.csv\"     # The training set\n",
    "test_csv = \"../data/quality/test.csv\"          # The test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dd81d",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The data that we'll be working with was manually annotated to label LC-MS datasets on several Orbitrap instruments as either good or poor. Here is how [Amidan et al](https://pubs.acs.org/doi/10.1021/pr401143e) describe the annotation process:\n",
    "> The data sets were manually reviewed by three expert instrument operators (30+ years of combined LC–MS experience) using an in-house graphical user interface viewer. This viewer contained the base peak chromatogram, total ion current chromatogram, plots of both the top 50 000 and top 500 000 LC–MS detected features, and the number of peptides identified. In the first round, 1150 data sets were manually curated as “good”, “okay”, or “poor” and used to develop the classifier. In cases where the assessors disagreed (∼5–10%), the majority opinion was taken for the curated value. Moreover, the “okay” value was used to denote the wide range of performance, which, although not optimal, was still acceptable. \n",
    "\n",
    "This annoation process resulted in the following data thate we'll be using. Run the code cell below to see an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the metadata table:\n",
    "pd.read_csv(metadata_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070010b",
   "metadata": {},
   "source": [
    "Four our model, we're going to focus on the Velo Orbitrap data, because that has the most examples. We must model them each individually, because they have different features. Let's load the training data and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313025f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv) # Read the training data.\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80a8f4",
   "metadata": {},
   "source": [
    "Notice the columns with `NaN` features. These indicate features that are missing for a particular instrument model. Now we'll filter the data for the Velos Orbitrap data and select the feature columns in the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only Velos Orbitrap data:\n",
    "train_velos = train_df.loc[train_df[\"Instrument_Category\"] == \"VOrbitrap\", :]\n",
    "\n",
    "# These columns are not features, so we want to ignore them:\n",
    "not_feature_columns = [\n",
    "    \"Instrument_Category\",\n",
    "    \"Instrument\",\n",
    "    \"Dataset_ID\",\n",
    "    \"Acq_Time_Start\",\n",
    "    \"Acq_Length\",\n",
    "    \"Dataset\",\n",
    "    \"Dataset_Type\",\n",
    "    \"Curated_Quality\",\n",
    "    \"BinomResp\",\n",
    "    \"LLRC.Prediction\",\n",
    "    # These features don't exist for this instrument:\n",
    "    \"MS1_TIC_Q2\",\n",
    "    \"MS1_TIC_Q3\",\n",
    "    \"C_1A\",\n",
    "    \"C_1B\",\n",
    "    \"C_2B\",\n",
    "    \"C_3B\",\n",
    "    \"C_4A\",\n",
    "    \"C_4C\",\n",
    "    \"P_2Anorm\",\n",
    "]\n",
    "\n",
    "# Find the feature columns in the data:\n",
    "# This is a Python feature called a \"list comprehension\"\n",
    "feature_columns = [c for c in train_velos.columns if c not in not_feature_columns]\n",
    "\n",
    "# Extract the features and labels:\n",
    "X_train = train_velos.loc[:, feature_columns]\n",
    "y_train = train_velos.loc[:, \"BinomResp\"]\n",
    "\n",
    "# Print the size of our dataset:\n",
    "print(\"We have\", X_train.shape[1], \"features\")\n",
    "print(\"and\", X_train.shape[0], \"examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d8924",
   "metadata": {},
   "source": [
    "Note that here is where we would normally perform data exploration, making plots of everything we think might be relevant and identifying potential problems with data quality. We're not going to do that here for the sake of time.\n",
    "\n",
    "## Normalize the features\n",
    "\n",
    "It is important to scale (transform all of the features so that they are of the same magnitude) and center (transform all of the features so that their mean is 0) the features of our dataset in order for many machine learning methods to work optimally. This can be easily accomplished for a feature by subtracting the mean and dividing by the standard deviation. This is often called *standardization* or *standard scaling*, because our features will now be similar to a standard normal distribution–they will have a standar deviation of 1 and mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features:\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4caf2",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model\n",
    "\n",
    "We're now ready to fit our logistic regression model. Just as in the paper, we're going to use L1 regularization to perform implicit feature selection—recall that L1 regularization forces small coefficients to go to 0, essentially removing unimportant features from the model. We'll use 3-fold cross-validation to select how much L1 regularization we need, which is a hyperparameter we need to select.\n",
    "\n",
    "Fortunately, the `LogisticRegressionCV` model in scikit-learn can take care of the cross-validation for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = LogisticRegressionCV(\n",
    "    penalty=\"l1\",       # Use L1 regularization\n",
    "    cv=3,               # Use 3 cross-validation folds.\n",
    "    scoring=\"roc_auc\",  # Use ROC AUC to choose which model is best.\n",
    "    solver=\"saga\",      # Needed to use L1 regularization.\n",
    "    max_iter=5000,      # Number of steps needed for the models to finish training.\n",
    ")\n",
    "\n",
    "# Fit the model:\n",
    "model.fit(X_standardized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8459cba",
   "metadata": {},
   "source": [
    "## Let's see how we did\n",
    "\n",
    "Now that we're all done with the modeling process, it's time to assess the performance of the model and look at what the it learned. Since we're done with modeling, it is time to load and predict on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_csv) # Read the test data.\n",
    "\n",
    "# Filter for only Velos Orbitrap data:\n",
    "test_velos = train_df.loc[train_df[\"Instrument_Category\"] == \"VOrbitrap\", :]\n",
    "\n",
    "# Extract the features and labels:\n",
    "X_test = test_velos.loc[:, feature_columns]\n",
    "y_test = test_velos.loc[:, \"BinomResp\"]\n",
    "\n",
    "# Scale and center our test data:\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "# Get the class predictions for the test set examples:\n",
    "y_pred = model.predict(X_test_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of our classifier on the test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", f'{100*accuracy_score(y_test, y_pred):.{4}}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413995b0",
   "metadata": {},
   "source": [
    "Two commonly used plots used to asses the performance of a binary classifier are receiver operating characteristic (ROC) curves and precision-recall curves. \n",
    "\n",
    "An `ROC curve` provides a more comprehensive view of performance than accuracy, since it describes the tradeoff between false positive rate and true positive rate across the full range of predicted probabilities. It allows you to evaluate how well the accuracy of predictions correspond to it's assigned confidence p(X). \n",
    "\n",
    "A `precision-recall` curve summarizes the trade-off between the true positive rate and the sensitivity (the proportion of true labels predicted) for a classifier using different probability thresholds. While it conveys similar information to an ROC curve, it is less sensitive to the number of data points in each class so is a good choice for inbalanced datasets  \n",
    "\n",
    "How do these ROC and precision-recall curves look to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e19ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities predicted for the test set examples:\n",
    "probs = model.predict_proba(X_test_standardized)[:, 1]\n",
    "\n",
    "# Create our blank figure with 2 panels\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot the ROC and PR curves:\n",
    "RocCurveDisplay.from_predictions(y_test, probs, ax=axs[0])\n",
    "PrecisionRecallDisplay.from_predictions(y_test, probs, ax=axs[1])\n",
    "\n",
    "# Display the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2fdb9",
   "metadata": {},
   "source": [
    "**Homework:** How do our results compare to that presented in the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try an SVM model \n",
    "\n",
    "It's possible that the relationship between some of our features and spectral quality is non-linear. With an SVM using an RBF kernel, we can capture this non-linearity and potentially reduce the bias of our model. However, this added complexity has the potential to increase the variance if we start overfitting to the training data. Let's see what happens! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/justinsanders/Documents/noble lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize the SVM model:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m SVC(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     probability\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,   \u001b[39m# Have the model output probabilities rather than binary labels (so we can plot ROC curves) \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m,       \u001b[39m# We want to use an RBF kernel to allow the SVM to capture non-linear relationships\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     gamma\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,            \u001b[39m# Our data is standardized and thus has variance 1 \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m1966\u001b[39m   \u001b[39m# Set the random seed to make training reproducible  \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Fit the model:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justinsanders/Documents/noble%20lab/ushupo-ml-short-course/notebooks/2.1_logistic-regression.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_standardized, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM model:\n",
    "model = SVC(\n",
    "    probability=True,   # Have the model output probabilities rather than binary labels (so we can plot ROC curves) \n",
    "    kernel='rbf',       # We want to use an RBF kernel to allow the SVM to capture non-linear relationships\n",
    "    gamma=1,            # Our data is standardized and thus has variance 1 \n",
    "    random_state=1966   # Set the random seed to make training reproducible  \n",
    ")\n",
    "\n",
    "# Fit the model:\n",
    "model.fit(X_standardized, y_train)\n",
    "\n",
    "# Get the class predictions for the test set examples:\n",
    "y_pred = model.predict(X_test_standardized)\n",
    "print(\"Accuracy:\", f'{100*accuracy_score(y_test, y_pred):.{4}}%')\n",
    "\n",
    "# Get the probabilities predicted for the test set examples:\n",
    "probs = model.predict_proba(X_test_standardized)[:, 1]\n",
    "\n",
    "# Create our blank figure with 2 panels\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot the ROC and PR curves:\n",
    "RocCurveDisplay.from_predictions(y_test, probs, ax=axs[0])\n",
    "PrecisionRecallDisplay.from_predictions(y_test, probs, ax=axs[1])\n",
    "\n",
    "# Display the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the SVM compare to logistic regression?\n",
    "\n",
    " Despite the difference in performance, can you think of a reason one might still choose to use logistic regression for this task? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
